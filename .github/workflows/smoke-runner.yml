name: smoke / run-uaengine

on:
  push:
  pull_request:

permissions:
  contents: read

env:
  # Keep cache dirs in the workspace so actions/cache can persist them reliably
  UENG_CACHE_ROOT: ${{ github.workspace }}/.cache

jobs:
  # ------------------------------------------------------------
  # Build & run --version on all OSes using your runner scripts
  # ------------------------------------------------------------
  smoke-run:
    name: smoke (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ---------- Linux/macOS: Ninja + ccache ----------
      - name: Install Ninja & ccache (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y ninja-build ccache

      - name: Install Ninja & ccache (macOS)
        if: runner.os == 'macOS'
        run: |
          brew update
          brew install ninja ccache || true

      - name: Prepare ccache directory (Unix)
        if: runner.os != 'Windows'
        run: |
          mkdir -p "${UENG_CACHE_ROOT}/ccache"

      - name: Restore ccache (Unix)
        if: runner.os != 'Windows'
        uses: actions/cache@v4
        with:
          path: ${{ env.UENG_CACHE_ROOT }}/ccache
          key: ccache-${{ runner.os }}-${{ hashFiles('CMakeLists.txt', 'src/**', 'include/**', 'snippets/**', 'cmake/**') }}
          restore-keys: |
            ccache-${{ runner.os }}-

      - name: Configure + Build (Unix / Ninja + ccache)
        if: runner.os != 'Windows'
        shell: bash
        env:
          CCACHE_DIR: ${{ env.UENG_CACHE_ROOT }}/ccache
        run: |
          # Tell CMake to use ccache
          cmake -S . -B build-ninja \
            -G Ninja -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_C_COMPILER_LAUNCHER=ccache \
            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache
          cmake --build build-ninja -j
          ccache -s || true

      - name: Cache build folder (Unix; best-effort)
        if: runner.os != 'Windows'
        uses: actions/cache@v4
        with:
          path: build-ninja
          key: cmake-build-${{ runner.os }}-${{ github.ref }}-${{ hashFiles('CMakeLists.txt', 'src/**', 'include/**', 'snippets/**', 'cmake/**') }}
          restore-keys: |
            cmake-build-${{ runner.os }}-${{ github.ref }}-
            cmake-build-${{ runner.os }}-

      # ---------- Windows: MSBuild + cache build dir ----------
      - name: Configure + Build (Windows / MSBuild)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          cmake -S . -B build
          cmake --build build -j

      - name: Cache build folder (Windows; best-effort)
        if: runner.os == 'Windows'
        uses: actions/cache@v4
        with:
          path: build
          key: cmake-build-${{ runner.os }}-${{ github.ref }}-${{ hashFiles('CMakeLists.txt', 'src/**', 'include/**', 'snippets/**', 'cmake/**') }}
          restore-keys: |
            cmake-build-${{ runner.os }}-${{ github.ref }}-
            cmake-build-${{ runner.os }}-

      # ---------- Exercise runner scripts ----------
      - name: Run (Windows) — --version
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          $env:UENG_RUN_VERBOSE="1"
          powershell -ExecutionPolicy Bypass -File scripts\run-uaengine.ps1 -- --version

      - name: Run (Unix) — --version
        if: runner.os != 'Windows'
        shell: bash
        run: |
          export UENG_RUN_VERBOSE=1
          scripts/run-uaengine.sh -- --version

  # ------------------------------------------------------------------
  # Optional: OpenAI smoke. Only runs if OPENAI_API_KEY is present.
  # ------------------------------------------------------------------
  smoke-run-openai:
    if: ${{ secrets.OPENAI_API_KEY != '' }}
    name: smoke-openai (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      UENG_LLM_PROVIDER: openai
      UENG_CACHE_ROOT: ${{ github.workspace }}/.cache

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Linux: Ninja + ccache + OpenAI
      - name: Install Ninja & ccache (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y ninja-build ccache

      - name: Prepare ccache directory (Linux)
        if: runner.os == 'Linux'
        run: |
          mkdir -p "${UENG_CACHE_ROOT}/ccache"

      - name: Restore ccache (Linux)
        if: runner.os == 'Linux'
        uses: actions/cache@v4
        with:
          path: ${{ env.UENG_CACHE_ROOT }}/ccache
          key: ccache-openai-${{ runner.os }}-${{ hashFiles('CMakeLists.txt', 'src/**', 'include/**', 'snippets/**', 'cmake/**') }}
          restore-keys: |
            ccache-openai-${{ runner.os }}-

      - name: Configure + Build (Linux / Ninja + OpenAI)
        if: runner.os == 'Linux'
        shell: bash
        env:
          CCACHE_DIR: ${{ env.UENG_CACHE_ROOT }}/ccache
        run: |
          cmake -S . -B build-ninja -G Ninja -DCMAKE_BUILD_TYPE=Release \
                -DUAENG_ENABLE_OPENAI=ON \
                -DCMAKE_C_COMPILER_LAUNCHER=ccache \
                -DCMAKE_CXX_COMPILER_LAUNCHER=ccache
          cmake --build build-ninja -j
          ccache -s || true

      - name: Configure + Build (Windows / MSBuild + OpenAI)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          cmake -S . -B build -DUAENG_ENABLE_OPENAI=ON
          cmake --build build -j

      - name: Run (Windows) — llm-selftest (OpenAI)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          powershell -ExecutionPolicy Bypass -File scripts\run-uaengine.ps1 -- llm-selftest gpt-4o-mini

      - name: Run (Linux) — llm-selftest (OpenAI)
        if: runner.os == 'Linux'
        shell: bash
        run: |
          scripts/run-uaengine.sh llm-selftest gpt-4o-mini

  # ------------------------------------------------------------------
  # Optional: Ollama smoke (Docker). Enable using repo variable:
  # Settings → Actions → Variables → New variable:
  #   Name: UENG_ENABLE_OLLAMA_SMOKE   Value: true
  #
  # Caches the tiny model via actions/cache by bind-mounting a host dir.
  # ------------------------------------------------------------------
  smoke-run-ollama:
    if: ${{ vars.UENG_ENABLE_OLLAMA_SMOKE == 'true' }}
    name: smoke-ollama (ubuntu)
    runs-on: ubuntu-latest
    env:
      UENG_LLM_PROVIDER: ollama
      UENG_OLLAMA_HOST: http://localhost:11434
      OLLAMA_CACHE_DIR: ${{ github.workspace }}/.cache/ollama-models

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Ninja & tools
        run: |
          sudo apt-get update
          sudo apt-get install -y ninja-build jq curl

      - name: Prepare model cache dir
        run: |
          mkdir -p "${OLLAMA_CACHE_DIR}"

      - name: Restore Ollama model cache
        uses: actions/cache@v4
        with:
          path: ${{ env.OLLAMA_CACHE_DIR }}
          key: ollama-models-tinyllama
          restore-keys: |
            ollama-models-

      - name: Start Ollama (Docker with model bind-mount)
        run: |
          docker run -d --name ollama -p 11434:11434 \
            -v "${OLLAMA_CACHE_DIR}:/root/.ollama" \
            ollama/ollama:latest
          # Wait for API to come up
          for i in {1..60}; do
            if curl -fsS http://localhost:11434/api/tags >/dev/null; then
              echo "Ollama is up"; break
            fi
            echo "Waiting for Ollama... ($i)"; sleep 2
          done

      - name: Pull tiny model (cached)
        run: |
          docker exec ollama ollama pull tinyllama

      - name: Configure + Build (Ninja + OLLAMA)
        run: |
          cmake -S . -B build-ninja -G Ninja -DCMAKE_BUILD_TYPE=Release -DUAENG_ENABLE_OLLAMA=ON
          cmake --build build-ninja -j

      - name: Run — llm-selftest (Ollama)
        shell: bash
        run: |
          export UENG_RUN_VERBOSE=1
          scripts/run-uaengine.sh llm-selftest tinyllama

      - name: Docker logs (on failure)
        if: failure()
        run: |
          docker logs ollama || true

      - name: Save Ollama model cache
        if: always()
        uses: actions/cache/save@v4
        with:
          path: ${{ env.OLLAMA_CACHE_DIR }}
          key: ollama-models-tinyllama

      - name: Cleanup Docker
        if: always()
        run: |
          docker rm -f ollama || true
